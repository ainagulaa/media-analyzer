# -*- coding: utf-8 -*-
"""
Ğ¤ĞĞ™Ğ›: text_preprocessing.py

ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ:
    - Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ CSV-Ñ„Ğ°Ğ¹Ğ» Ñ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¾Ğ¹ 'text'
    - ĞÑ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ñ‹ (Ñ€ÑƒÑÑĞºĞ¸Ğ¹ + ĞºĞ°Ğ·Ğ°Ñ…ÑĞºĞ¸Ğ¹)
    - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ CSV Ñ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¾Ğ¹ 'cleaned_text'

ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:
    pip install pandas nltk
"""

import re
import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ğŸ”§ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² NLTK
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def download_nltk_resources():
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

    try:
        nltk.data.find("corpora/stopwords")
    except LookupError:
        nltk.download("stopwords")

download_nltk_resources()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ğŸ“Œ Ğ¡Ñ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ°
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

russian_stopwords = set(stopwords.words("russian"))

kazakh_stopwords = {
    "Ğ¼ĞµĞ½", "Ğ¶Ó™Ğ½Ğµ", "Ğ±Ñ–Ñ€", "Ğ¾ÑÑ‹", "ÑĞ¾Ğ»", "Ğ±Ò±Ğ»", "Ğ±Ğ°Ñ€", "ĞµĞºĞµĞ½", "ĞµÑ‚Ñ–Ğ¿", "Ò›Ğ°Ñ‚Ñ‚Ñ‹", "ĞµĞ´Ñ–", "Ğ±Ğ¾Ğ»Ğ´Ñ‹",
    "Ğ±Ğ¾Ğ»Ñ‹Ğ¿", "Ñ‚ÑƒÑ€Ğ°Ğ»Ñ‹", "ÑĞ¸ÑÒ›Ñ‚Ñ‹", "Ñ‚Ğ°Ò“Ñ‹", "ĞºÓ©Ğ¿", "Ğ¼ĞµĞ½Ñ–Ò£", "ÑĞµĞ½", "Ğ¾Ğ»", "Ñ‚Ğ°Ò“Ñ‹Ğ´Ğ°", "ĞµĞ´Ñ–Ò£", "ĞµĞ´Ñ–Ò£Ñ–Ğ·",
    "ÑĞ¾Ò£", "ÑĞ¾Ğ½Ñ‹Ğ¼ĞµĞ½", "Ğ´ĞµĞ³ĞµĞ½", "Ğ°Ñ€Ğ°ÑÑ‹Ğ½Ğ´Ğ°", "Ğ±ĞµÑ€ĞµĞ´Ñ–", "Ò›Ğ°Ğ»Ğ´Ñ‹", "Ğ´ĞµĞ³ĞµĞ½Ğ¼ĞµĞ½", "ĞºĞµÑ€ĞµĞº", "Ò›Ğ°Ğ¶ĞµÑ‚", "Ò›Ğ°Ğ½Ğ´Ğ°Ğ¹"
}

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# âœ¨ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def preprocess_text(text: str) -> str:
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)      # ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ğ¸
    text = re.sub(r"\d+", "", text)          # ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ†Ğ¸Ñ„Ñ€

    try:
        words = word_tokenize(text, language='russian')  # Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ·Ğ°Ñ…ÑĞºĞ¾Ğ³Ğ¾
    except:
        words = text.split()

    filtered_words = [
        word for word in words
        if word not in russian_stopwords and word not in kazakh_stopwords
    ]

    return " ".join(filtered_words)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° CSV
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def process_csv(input_path: str, output_path: str):
    try:
        df = pd.read_csv(input_path, encoding="utf-8")
    except FileNotFoundError:
        print(f"âŒ Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {input_path}")
        return

    if "text" not in df.columns:
        print("âŒ Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ 'text'")
        return

    print("ğŸ”„ ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²...")

    df["cleaned_text"] = df["text"].astype(str).apply(preprocess_text)

    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ²: {output_path}")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if __name__ == "__main__":
    input_csv = "data/articles.csv"
    output_csv = "data/articles_cleaned.csv"
    process_csv(input_csv, output_csv)
