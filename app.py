import streamlit as st
st.set_page_config(page_title="–ú–µ–¥–∏–∞-–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä", layout="wide")

import pandas as pd
import re
import nltk
import plotly.express as px
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from bs4 import BeautifulSoup
import requests
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from collections import defaultdict
import torch
import math

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
nltk.download("punkt")
nltk.download("stopwords")
stemmer = SnowballStemmer("russian")
ru_stop = set(stopwords.words("russian"))
kz_stop = {
    "–º–µ–Ω", "–∂”ô–Ω–µ", "–±—ñ—Ä", "–æ—Å—ã", "—Å–æ–ª", "–±“±–ª", "–±–∞—Ä", "–µ–∫–µ–Ω", "–µ—Ç—ñ–ø", "“õ–∞—Ç—Ç—ã", "–µ–¥—ñ", "–±–æ–ª–¥—ã",
    "–±–æ–ª—ã–ø", "—Ç—É—Ä–∞–ª—ã", "—Å–∏—è“õ—Ç—ã", "—Ç–∞“ì—ã", "–∫”©–ø", "–º–µ–Ω—ñ“£", "—Å–µ–Ω", "–æ–ª", "—Ç–∞“ì—ã–¥–∞", "–µ–¥—ñ“£", "–µ–¥—ñ“£—ñ–∑"
}

@st.cache_resource
def load_sentiment_model():
    tokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny-sentiment-balanced")
    model = AutoModelForSequenceClassification.from_pretrained("cointegrated/rubert-tiny-sentiment-balanced")
    return tokenizer, model

sent_tokenizer, sent_model = load_sentiment_model()


@st.cache_resource
def load_fake_news_model():
    tokenizer = AutoTokenizer.from_pretrained("tellowit/rubert-fake-news-classification")
    model = AutoModelForSequenceClassification.from_pretrained("tellowit/rubert-fake-news-classification")
    return tokenizer, model

fake_tokenizer, fake_model = load_fake_news_model()

def detect_fake_rubert(text):
    inputs = fake_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=512
    )
    with torch.no_grad():
        outputs = fake_model(**inputs)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=1)
    fake_conf = probs[0][1].item()  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ —ç—Ç–æ —Ñ–µ–π–∫

    if fake_conf > 0.85:
        return f"üö® –§–µ–π–∫ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {fake_conf:.0%})"
    elif fake_conf > 0.6:
        return f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ —Ñ–µ–π–∫ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {fake_conf:.0%})"
    elif fake_conf > 0.4:
        return f"‚ùì –°–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {fake_conf:.0%})"
    else:
        return f"‚úÖ –ù–∞–¥—ë–∂–Ω—ã–π (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {1 - fake_conf:.0%})"

def clean(text):
    text = str(text).lower()
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"\d+", "", text)
    tokens = text.split()
    return " ".join([w for w in tokens if w not in ru_stop and w not in kz_stop])

def estimate_influence(text, views=None, likes=None, shares=None):
    text = text.lower()
    words = text.split()
    word_count = len(words)

    if word_count == 0:
        return "üü¢ –ù–∏–∑–∫–æ–µ"

    # –°—Ç–µ–º–º–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
    stemmed_text = [stemmer.stem(word) for word in words]

    # –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    high_risk = ["–≤–∑—Ä—ã–≤", "—Ç–µ—Ä—Ä–æ—Ä", "–º–∏—Ç–∏–Ω–≥", "—Å–∞–Ω–∫—Ü–∏", "–≤–æ–µ–Ω–Ω", "—Å—Ç—Ä–µ–ª—å–±", "–∞–≤–∞—Ä", "—É–≥—Ä–æ–∑", "–ø–∞–Ω–∏–∫", "—Ç–µ—Ä–∞–∫—Ç", "—É–±–∏–π—Å—Ç–≤", "–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ"]
    medium_risk = ["—Ä–µ—Ñ–æ—Ä–º", "–∑–∞–∫–æ–Ω", "–∏–Ω—Ñ–ª—è—Ü", "–∫—Ä–∏–∑–∏—Å", "—à—Ç—Ä–∞—Ñ", "—Å—É–¥", "–ø–µ—Ç–∏—Ü–∏", "–∂–∞–ª–æ–±", "–∞—Ä–µ—Å—Ç", "—Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω", "–æ–±–æ—Å—Ç—Ä–µ–Ω", "–¥–µ—Ñ–∏—Ü–∏—Ç"]
    emotional = ["–≥–Ω–µ–≤", "—è—Ä–æ—Å—Ç", "—à–æ–∫", "–≤–æ—Å—Ç–æ—Ä–≥", "–≤–æ–∑–º—É—â–µ–Ω", "—Å–∫–∞–Ω–¥–∞–ª", "—Ç—Ä–µ–±", "–æ–±–≤–∏–Ω", "–Ω–µ–≥–æ–¥–æ–≤–∞–Ω", "–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤", "–∂–µ—Å—Ç–∫", "—Ä–µ–∑–∫", "–ø–æ–∑–æ—Ä"]
    clickbait = ["—à–æ–∫", "–Ω–µ–æ–∂–∏–¥", "–≤–∑–æ—Ä–≤–∞–ª", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω", "—ç–∫—Å–∫–ª—é–∑–∏–≤", "—Å—Ä–æ—á–Ω", "—à–æ–∫–∏—Ä—É—é—â"]

    # –ü–æ–¥—Å—á—ë—Ç—ã
    def count_matches(stemmed_list, category):
        return sum(1 for word in stemmed_list if any(word.startswith(trigger) for trigger in category))

    score = 0
    score += count_matches(stemmed_text, high_risk) * 3
    score += count_matches(stemmed_text, medium_risk) * 2
    score += count_matches(stemmed_text, emotional) * 1.5
    score += count_matches(stemmed_text, clickbait) * 1.2

    # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    total_triggers = count_matches(stemmed_text, high_risk + medium_risk + emotional)
    density = total_triggers / word_count
    score += density * 10  # –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
    score += 1 if text.count("!") > 2 else 0
    score += 0.5 if text.count("?") > 2 else 0
    score += math.log(word_count + 1, 10)

    # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    if views and views > 10000:
        score += 1.5
    if likes and likes > 500:
        score += 1
    if shares and shares > 100:
        score += 1

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
    if score >= 12:
        return "üö® –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ"
    elif score >= 8:
        return "üî∫ –í—ã—Å–æ–∫–æ–µ"
    elif score >= 4:
        return "‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ–µ"
    else:
        return "üü¢ –ù–∏–∑–∫–æ–µ"

def classify_topic(text):
    text = text.lower()

    topic_keywords = {
        "‚öñÔ∏è –ü–æ–ª–∏—Ç–∏–∫–∞": {
            "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç": 3, "–¥–µ–ø—É—Ç–∞—Ç": 3, "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç": 3, "–∑–∞–∫–æ–Ω": 3, "–≤—ã–±–æ—Ä": 2,
            "–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏": 2, "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤": 3, "–ø–∞—Ä—Ç–∏–π": 2, "–º–∏—Ç–∏–Ω–≥": 3, "–æ–ø–ø–æ–∑–∏—Ü–∏": 2
        },
        "üí∞ –≠–∫–æ–Ω–æ–º–∏–∫–∞": {
            "—ç–∫–æ–Ω–æ–º–∏–∫": 3, "–∏–Ω—Ñ–ª—è—Ü": 3, "–∫—É—Ä—Å": 2, "–±—é–¥–∂–µ—Ç": 2, "–¥–µ–Ω—å–≥": 1,
            "—Ñ–∏–Ω–∞–Ω—Å": 3, "–Ω–∞–ª–æ–≥": 2, "–±–∞–Ω–∫": 2, "–∑–∞—Ä–ø–ª–∞—Ç": 2, "—Ü–µ–Ω": 1
        },
        "üî¨ –ù–∞—É–∫–∞/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": {
            "–Ω–∞—É–∫": 3, "–∏–Ω–Ω–æ–≤–∞—Ü": 3, "—Ç–µ—Ö–Ω–æ–ª–æ–≥": 3, "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω": 3, "–∞–ª–≥–æ—Ä–∏—Ç–º": 2,
            "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä": 2, "—Ä–æ–±–æ—Ç": 2, "–ò–ò": 3, "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω": 3, "–Ω–µ–π—Ä–æ—Å–µ—Ç": 2
        },
        "üåç –≠–∫–æ–ª–æ–≥–∏—è": {
            "—ç–∫–æ–ª–æ–≥": 3, "–ø—Ä–∏—Ä–æ–¥": 2, "–∫–ª–∏–º–∞—Ç": 3, "—É–≥–ª–µ—Ä–æ–¥": 2, "–∑–∞–≥—Ä—è–∑–Ω": 3,
            "–æ–∫—Ä—É–∂–∞—é—â": 2, "—Å–≤–∞–ª–∫": 2, "–≤–æ–¥–æ—ë–º": 1, "–≥–ª–æ–±–∞–ª—å–Ω –ø–æ—Ç–µ–ø–ª–µ–Ω": 3
        },
        "üéì –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ": {
            "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏": 3, "—à–∫–æ–ª": 3, "—É—á–µ–±": 2, "—Å—Ç—É–¥–µ–Ω—Ç": 2, "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç": 2,
            "—ç–∫–∑–∞–º–µ–Ω": 1, "—É—á–∏—Ç–µ–ª": 1, "—Ä–µ—Ñ–æ—Ä–º–∞": 1, "–∞–∫–∞–¥–µ–º": 1
        },
        "üßë‚Äç‚öïÔ∏è –ó–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ": {
            "–º–µ–¥–∏—Ü–∏–Ω": 3, "–≤–∞–∫—Ü–∏–Ω": 3, "–≤—Ä–∞—á": 2, "–ª–µ—á–µ–Ω": 2, "—ç–ø–∏–¥–µ–º–∏": 2,
            "–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω": 3, "–ø–∞—Ü–∏–µ–Ω—Ç": 1, "–≤–∏—Ä—É—Å": 2, "–±–æ–ª–µ–∑–Ω": 2, "—Å–∏–º–ø—Ç–æ–º": 1
        },
        "üïäÔ∏è –û–±—â–µ—Å—Ç–≤–æ": {
            "—Å–æ—Ü–∏–∞–ª—å–Ω": 2, "—Å–µ–º—å": 2, "–ø–µ–Ω—Å–∏–æ–Ω–µ—Ä": 2, "–º–æ–ª–æ–¥–µ–∂": 2, "–∏–Ω–∫–ª—é–∑": 1,
            "—Ä–∞–≤–µ–Ω—Å—Ç–≤": 2, "–æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω": 2, "–Ω–∞—Å–µ–ª–µ–Ω": 1, "–≥—Ä–∞–∂–¥–∞–Ω": 1
        },
        "üé≠ –ö—É–ª—å—Ç—É—Ä–∞/–ò—Å–∫—É—Å—Å—Ç–≤–æ": {
            "–∫—É–ª—å—Ç—É—Ä": 3, "–∏—Å–∫—É—Å—Å—Ç–≤": 3, "–º—É–∑–µ": 2, "—Ç–µ–∞—Ç—Ä": 2, "—Ñ–µ—Å—Ç–∏–≤–∞–ª—å": 1,
            "—Ç—Ä–∞–¥–∏—Ü": 2, "–Ω–∞—Å–ª–µ–¥–∏": 2, "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä": 1, "–ø—Ä–µ–º—å": 1, "–∫–∏–Ω–æ": 2
        },
        "üì∞ –°–ú–ò/–ú–µ–¥–∏–∞": {
            "–∂—É—Ä–Ω–∞–ª–∏—Å—Ç": 3, "—Ä–µ–ø–æ—Ä—Ç": 2, "—Ü–µ–Ω–∑—É—Ä": 2, "—Å–º–∏": 3, "–∏–Ω—Ç–µ—Ä–≤—å—é": 1,
            "–Ω–æ–≤–æ—Å—Ç–Ω": 2, "—Ä–µ–¥–∞–∫—Ü": 2, "—Å–æ—Ü—Å–µ—Ç": 2, "–º–µ–¥–∏–∞": 2, "–∫–∞–Ω–∞–ª": 1
        },
        "‚öΩ –°–ø–æ—Ä—Ç": {
            "—Å–ø–æ—Ä—Ç": 3, "—Ñ—É—Ç–±–æ–ª": 3, "–º–∞—Ç—á": 2, "–∫–æ–º–∞–Ω–¥": 2, "–∏–≥—Ä–æ–∫": 2,
            "—á–µ–º–ø–∏–æ–Ω–∞—Ç": 2, "—Ç—É—Ä–Ω–∏—Ä": 2, "–æ–ª–∏–º–ø–∏–∞–¥": 2, "—Ç—Ä–µ–Ω–µ—Ä": 1
        },
        "üö® –ü—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è": {
            "–ø–æ–∂–∞—Ä": 3, "–∞–≤–∞—Ä": 3, "–¥—Ç–ø": 3, "—É–±–∏–π—Å—Ç–≤": 3, "–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ": 2,
            "—á–ø": 2, "–Ω–∞–µ–∑–¥": 1, "–∑–∞–¥–µ—Ä–∂–∞–Ω": 2, "–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω": 2, "–∏–Ω—Ü–∏–¥–µ–Ω—Ç": 1
        },
        "üåê –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –¥–µ–ª–∞": {
            "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω": 3, "—Å–∞–Ω–∫—Ü": 3, "–∫–æ–Ω—Ñ–ª–∏–∫—Ç": 3, "–≤—Ç–æ—Ä–∂–µ–Ω": 3, "–≤–æ–π–Ω": 3,
            "–Ω–∞—Ç–æ": 2, "—Å—à–∞": 2, "—É–∫—Ä–∞–∏–Ω": 3, "–µ–≤—Ä–æ—Å–æ—é–∑": 2, "–ø—É—Ç–∏–Ω": 2, "–±–∞–π–¥–µ–Ω": 2
        }
    }

    scores = defaultdict(int)

    for topic, keywords in topic_keywords.items():
        for root, weight in keywords.items():
            if re.search(rf"\b{root}\w*", text):  # –∏—â–µ—Ç –ø–æ –∫–æ—Ä–Ω—è–º
                scores[topic] += weight

    if not scores:
        return "üì¶ –ü—Ä–æ—á–µ–µ"

    max_score = max(scores.values())
    candidates = [t for t, s in scores.items() if s == max_score]

    if len(candidates) == 1:
        return candidates[0]

    # –†–∞–∑—Ä–µ—à–∞–µ–º —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ ‚Äî –≤—ã–±–∏—Ä–∞–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª—é—á–µ–π
    key_matches = {
        topic: sum(1 for word in topic_keywords[topic] if re.search(rf"\b{word}\w*", text))
        for topic in candidates
    }

    return max(key_matches, key=key_matches.get)


def analyze_sentiment(text):
    positive = ["–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ", "—É—Å–ø–µ—Ö", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Ä–æ—Å—Ç", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞"]
    negative = ["–∫—Ä–∏–∑–∏—Å", "–ø—Ä–æ—Ç–µ—Å—Ç", "–ø—Ä–æ–±–ª–µ–º–∞", "–∏–Ω—Ü–∏–¥–µ–Ω—Ç", "—É–≥—Ä–æ–∑–∞"]
    pos_count = sum(1 for word in positive if word in text)
    neg_count = sum(1 for word in negative if word in text)
    if pos_count > neg_count:
        return "üôÇ –ü–æ–∑–∏—Ç–∏–≤"
    elif neg_count > pos_count:
        return "‚òπÔ∏è –ù–µ–≥–∞—Ç–∏–≤"
    else:
        return "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ"

def analyze_sentiment_model(text):
    inputs = sent_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=512
    )
    with torch.no_grad():
        outputs = sent_model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    label = torch.argmax(probs).item()

    if label == 0:
        return "‚òπÔ∏è –ù–µ–≥–∞—Ç–∏–≤"
    elif label == 1:
        return "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    else:
        return "üôÇ –ü–æ–∑–∏—Ç–∏–≤"
    
def hybrid_sentiment(tone_heuristic, tone_model):
    if tone_heuristic == tone_model:
        return tone_model  # –°–æ–≤–ø–∞–¥–∞—é—Ç ‚Äî –±–µ—Ä—ë–º –ª—é–±—É—é
    elif tone_model != "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ":
        return tone_model  # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞ ‚Äî –¥–æ–≤–µ—Ä—è–µ–º –µ–π
    else:
        return tone_heuristic  # –ò–Ω–∞—á–µ fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É



def fetch_article(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/90 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        text = " ".join(p.get_text(strip=True) for p in paragraphs)
        return text if len(text) > 100 else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ."
    except:
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—å—é."

def visualize_clusters(X, labels, title, label_map=None):
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(X.toarray())
    
    # –µ—Å–ª–∏ label_map –ø–µ—Ä–µ–¥–∞–Ω ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if label_map:
        named_labels = pd.Series(labels).map(label_map)
    else:
        named_labels = pd.Series(labels)

    df_vis = pd.DataFrame({
        "PCA1": reduced[:, 0],
        "PCA2": reduced[:, 1],
        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": named_labels
    })

    fig = px.scatter(
        df_vis, x="PCA1", y="PCA2", color="–ö–∞—Ç–µ–≥–æ—Ä–∏—è",
        title=title, template="plotly_white", height=500,
        color_discrete_sequence=px.colors.qualitative.Set1
    )
    st.plotly_chart(fig, use_container_width=True)

# ‚Äî‚Äî‚Äî –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ‚Äî‚Äî‚Äî
st.title("üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è –º–µ–¥–∏–∞-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–Ω–µ–Ω–∏–µ")
mode = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ["üîé –í—Å—Ç–∞–≤–∏—Ç—å URL —Å—Ç–∞—Ç—å–∏", "üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç—å CSV-—Ñ–∞–π–ª"])

if mode == "üîé –í—Å—Ç–∞–≤–∏—Ç—å URL —Å—Ç–∞—Ç—å–∏":
    url = st.text_input("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é")
    if url:
        article_text = fetch_article(url)
        st.text_area("–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏", article_text[:5000], height=300)

        cleaned = clean(article_text)
        topic = classify_topic(cleaned)
        tone = analyze_sentiment(cleaned)
        model_tone = analyze_sentiment_model(cleaned)
        st.markdown(f"**ü§ñ –ú–æ–¥–µ–ª—å–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {model_tone}")
        if model_tone != tone:
            st.markdown("‚ö†Ô∏è –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ **—Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è**")
        else:
            st.markdown("‚úÖ –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ **—Å–æ–≤–ø–∞–¥–∞—é—Ç**")
        hybrid_tone = hybrid_sentiment(tone, model_tone)
        st.markdown(f"**üß™ –ì–∏–±—Ä–∏–¥–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {hybrid_tone}")

        infl = estimate_influence(article_text)
        fake_flag = detect_fake_rubert(article_text)

        st.markdown(f"**üß≠ –¢–µ–º–∞—Ç–∏–∫–∞:** {topic}")
        st.markdown(f"**üòê –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {tone}")
        st.markdown(f"**üì£ –í–ª–∏—è–Ω–∏–µ:** {infl}")
        st.markdown(f"**üö© –§–µ–π–∫–æ–≤–æ—Å—Ç—å:** {fake_flag}")

elif mode == "üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç—å CSV-—Ñ–∞–π–ª":
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –∫–æ–ª–æ–Ω–∫–æ–π 'text'", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        if "text" not in df.columns:
            st.error("CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'text'")
        else:
            with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º..."):
                df["cleaned_text"] = df["text"].apply(clean)
                tfidf = TfidfVectorizer(max_features=1000)
                X = tfidf.fit_transform(df["cleaned_text"])

                # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º (12 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)
                topic_model = KMeans(n_clusters=12, random_state=42)
                df["topic_cluster"] = topic_model.fit_predict(X)
                df["topic_label"] = "üß© –ö–ª–∞—Å—Ç–µ—Ä " + df["topic_cluster"].astype(str)

                # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                tone_model = KMeans(n_clusters=3, random_state=2)
                df["tone_cluster"] = tone_model.fit_predict(X)
                tone_map = {
                    0: "üôÇ –ü–æ–∑–∏—Ç–∏–≤",
                    1: "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
                    2: "‚òπÔ∏è –ù–µ–≥–∞—Ç–∏–≤"
                }
                df["tone_label"] = df["tone_cluster"].map(tone_map)

                # –û—Å—Ç–∞–ª—å–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
                df["influence"] = df["text"].apply(estimate_influence)
                df["fake_flag"] = df["text"].apply(detect_fake_rubert)

                tone_labels = {
                    0: "üôÇ –ü–æ–∑–∏—Ç–∏–≤",
                    1: "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
                    2: "‚òπÔ∏è –ù–µ–≥–∞—Ç–∏–≤"
                }

                df["topic_label"] = df["cleaned_text"].apply(classify_topic)
                df["tone_label"] = df["tone_cluster"].map(tone_labels)
                df["tone_model"] = df["cleaned_text"].apply(analyze_sentiment_model)
                df["hybrid_tone"] = df.apply(lambda row: hybrid_sentiment(row["tone_label"], row["tone_model"]), axis=1)
                df["tone_match"] = df["tone_model"] == df["tone_label"]


                sil_score = silhouette_score(X, df["topic_cluster"])
                calinski = calinski_harabasz_score(X.toarray(), df["topic_cluster"])
                davies = davies_bouldin_score(X.toarray(), df["topic_cluster"])

                metrics_df = pd.DataFrame({
                    "–ú–µ—Ç—Ä–∏–∫–∞": ["Silhouette Score", "Calinski-Harabasz Index", "Davies-Bouldin Index"],
                    "–ó–Ω–∞—á–µ–Ω–∏–µ": [round(sil_score, 4), round(calinski, 2), round(davies, 4)]
                })

                st.markdown("### üìê –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                st.dataframe(metrics_df, use_container_width=True)

                # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
                explanations = []

                if sil_score >= 0.5:
                    explanations.append(f"‚Ä¢ **Silhouette Score** = {sil_score:.4f}: –æ—Ç–ª–∏—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.")
                elif sil_score >= 0.3:
                    explanations.append(f"‚Ä¢ **Silhouette Score** = {sil_score:.4f}: –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
                else:
                    explanations.append(f"‚Ä¢ **Silhouette Score** = {sil_score:.4f}: —Å–ª–∞–±–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.")

                if calinski >= 100:
                    explanations.append(f"‚Ä¢ **Calinski-Harabasz Index** = {calinski:.2f}: –≤—ã—Å–æ–∫–æ–µ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, —Ö–æ—Ä–æ—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞.")
                else:
                    explanations.append(f"‚Ä¢ **Calinski-Harabasz Index** = {calinski:.2f}: —É–º–µ—Ä–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞.")

                if davies <= 1:
                    explanations.append(f"‚Ä¢ **Davies-Bouldin Index** = {davies:.3f}: –∫–ª–∞—Å—Ç–µ—Ä—ã —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∏ –ø–ª–æ—Ç–Ω—ã–µ.")
                else:
                    explanations.append(f"‚Ä¢ **Davies-Bouldin Index** = {davies:.3f}: –≤–æ–∑–º–æ–∂–Ω—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∏–ª–∏ —Ä–∞–∑–º—ã—Ç—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏.")

                st.markdown("### üß† –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫:")
                for line in explanations:
                    st.markdown(line)


            st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")
            st.subheader("üì£ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –º–µ–¥–∏–∞-–∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            influence_counts = df["influence"].value_counts().reset_index()
            influence_counts.columns = ["–í–ª–∏—è–Ω–∏–µ", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"]

            fig_inf = px.bar(
                influence_counts,
                x="–í–ª–∏—è–Ω–∏–µ",
                y="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
                color="–í–ª–∏—è–Ω–∏–µ",
                color_discrete_map={
                    "–≤—ã—Å–æ–∫–æ–µ": "crimson",
                    "—Å—Ä–µ–¥–Ω–µ–µ": "orange",
                    "–Ω–∏–∑–∫–æ–µ": "green"
                },
                text="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
                title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ –≤–ª–∏—è–Ω–∏—è",
            )
            fig_inf.update_layout(xaxis_title=None, yaxis_title="–ö–æ–ª-–≤–æ —Å—Ç–∞—Ç–µ–π", title_x=0.5)
            st.plotly_chart(fig_inf, use_container_width=True)
            match_rate = df["tone_match"].mean()
            st.markdown(f"### üîç –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –∏ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: **{match_rate:.1%}**")


            st.dataframe(df[["text", "topic_label", "tone_label", "tone_model", "hybrid_tone", "tone_match", "influence", "fake_flag"]], use_container_width=True)
            csv_bytes = df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", csv_bytes, "articles_final.csv", "text/csv")

            st.markdown("### üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            st.markdown("#### –¢–µ–º–∞—Ç–∏–∫–∞")
            visualize_clusters(X, df["topic_label"], "–¢–µ–º–∞—Ç–∏–∫–∞", None)
            st.markdown("#### –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å")
            visualize_clusters(X, df["tone_cluster"], "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", tone_labels)